{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# NOTEBOOK Voil\u00e0-friendly (Binder)\n# - Nessuna dipendenza da google.colab\n# - Niente /content: usa working dir\n# - Link di download via \"files/<nome>.zip\"\n\nimport os, re, time, csv, io, zipfile, shutil, traceback\nfrom urllib.parse import urljoin, urlparse\nimport requests\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nfrom bs4 import BeautifulSoup\n\nimport ipywidgets as widgets\nfrom IPython.display import display, HTML\n\nBASE_URL   = \"https://www.creokitchens.it/it/cucine\"\nSITE_ROOT  = \"https://www.creokitchens.it\"\nOUTPUT_DIR = \"./creo_cucine\"           # cartella locale\nZIP_PATH   = \"./creo_cucine.zip\"       # file locale\n\nUSER_AGENT = \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122 Safari/537.36\"\nREQUEST_TIMEOUT = (10, 25)\nSLEEP_BETWEEN_REQUESTS = 0.3\nMAX_IMAGE_BYTES = 40 * 1024 * 1024\nGLOBAL_PER_PAGE_TIMEOUT = 120\n\ndef build_session():\n    s = requests.Session()\n    s.headers.update({\"User-Agent\": USER_AGENT})\n    retries = Retry(total=4, connect=4, read=4, backoff_factor=0.5,\n                    status_forcelist=[429,500,502,503,504],\n                    allowed_methods=frozenset([\"GET\",\"HEAD\"]))\n    adapter = HTTPAdapter(max_retries=retries, pool_connections=10, pool_maxsize=10)\n    s.mount(\"http://\", adapter); s.mount(\"https://\", adapter)\n    return s\n\nsession = build_session()\n\ndef slugify(text, maxlen=80):\n    text = re.sub(r\"\\s+\", \" \", text or \"\").strip()\n    text = text.replace(\"/\", \"-\").replace(\"\\\\\", \"-\")\n    text = re.sub(r\"[^0-9A-Za-z\u00c0-\u00d6\u00d8-\u00f6\u00f8-\u00ff _\\-\\.\\(\\)]\", \"\", text)\n    text = text[:maxlen]\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    return text or \"senza_nome\"\n\ndef get_soup(url):\n    try:\n        r = session.get(url, timeout=REQUEST_TIMEOUT)\n        if r.status_code != 200: return None\n        return BeautifulSoup(r.text, \"html.parser\")\n    except requests.RequestException:\n        return None\n\ndef is_kitchen_detail_url(href_abs: str) -> bool:\n    try:\n        u = urlparse(href_abs)\n        if u.netloc != urlparse(SITE_ROOT).netloc or u.query or u.fragment: return False\n        path = u.path\n        if not path.startswith(\"/it/cucine/\"): return False\n        seg = [s for s in path.split(\"/\") if s]\n        if len(seg) != 3: return False\n        if \".\" in seg[-1]: return False\n        return True\n    except Exception:\n        return False\n\ndef head_ok_image(url):\n    try:\n        hr = session.head(url, timeout=REQUEST_TIMEOUT, allow_redirects=True)\n        if hr.status_code >= 400: return False\n        ct = (hr.headers.get(\"Content-Type\") or \"\").lower()\n        if \"image\" not in ct: return False\n        clen = hr.headers.get(\"Content-Length\")\n        if clen and clen.isdigit() and int(clen) > MAX_IMAGE_BYTES: return False\n        return True\n    except requests.RequestException:\n        return False\n\ndef infer_ext(url):\n    path = urlparse(url).path\n    ext = os.path.splitext(path)[1]\n    return ext.split(\"?\")[0] if (ext and len(ext) <= 5) else \".jpg\"\n\ndef download_image(url, dest_path):\n    if not head_ok_image(url): return False\n    try:\n        with session.get(url, stream=True, timeout=REQUEST_TIMEOUT) as r:\n            r.raise_for_status()\n            os.makedirs(os.path.dirname(dest_path), exist_ok=True)\n            total = 0\n            with open(dest_path, \"wb\") as f:\n                for chunk in r.iter_content(chunk_size=1024*64):\n                    if not chunk: continue\n                    f.write(chunk); total += len(chunk)\n                    if total > MAX_IMAGE_BYTES: return False\n        return True\n    except requests.RequestException:\n        return False\n\n# ---------- UI ----------\nout = widgets.Output()\n\ndef log(s):\n    with out:\n        print(s)\n\ndef list_kitchens():\n    listing = get_soup(BASE_URL)\n    if not listing: return []\n    slug2url = {}\n    for a in listing.select(\"a.gb-item-link\"):\n        href = a.get(\"href\") or \"\"\n        abs_url = urljoin(BASE_URL, href)\n        if is_kitchen_detail_url(abs_url):\n            slug = urlparse(abs_url).path.rstrip(\"/\").split(\"/\")[-1]\n            slug2url.setdefault(slug, abs_url)\n    resolved = []\n    for slug, url in slug2url.items():\n        s = get_soup(url); time.sleep(0.02)\n        name = slugify(s.find(\"h1\").get_text(strip=True)) if (s and s.find(\"h1\")) else slugify(slug)\n        resolved.append((name, url))\n    resolved.sort(key=lambda t: t[0].lower())\n    return resolved\n\n# Costruzione UI\nresolved = list_kitchens()\ncheckboxes = [widgets.Checkbox(value=False, description=name, indent=False) for name, _ in resolved]\nselect_all = widgets.ToggleButton(value=False, description=\"Seleziona/Deseleziona tutto\", icon=\"check\")\nimg_num = widgets.BoundedIntText(value=3, min=1, max=99, step=1, description=\"Immagini/cucina:\")\nbtn_run = widgets.Button(description=\"Avvia scraping\", button_style=\"success\", icon=\"play\")\ndownload_area = widgets.HTML(\"\")  # link di download comparir\u00e0 qui\n\nbox_checks = widgets.GridBox(\n    checkboxes,\n    layout=widgets.Layout(grid_template_columns=\"repeat(2, 48%)\", grid_gap=\"6px\")\n)\n\ndef on_toggle_all(change):\n    for cb in checkboxes:\n        cb.value = select_all.value\nselect_all.observe(on_toggle_all, 'value')\n\ndef run_scraping(_):\n    # reset area download\n    download_area.value = \"\"\n    out.clear_output()\n\n    try:\n        selected = [name for cb, (name, url) in zip(checkboxes, resolved) if cb.value]\n        if not selected:\n            log(\"Seleziona almeno una cucina.\"); \n            return\n\n        # pulizia\n        if os.path.exists(OUTPUT_DIR): shutil.rmtree(OUTPUT_DIR)\n        os.makedirs(OUTPUT_DIR, exist_ok=True)\n        if os.path.exists(ZIP_PATH): os.remove(ZIP_PATH)\n\n        log(f\"Avvio: {len(selected)} cucine | {img_num.value} immagini/cucina\")\n\n        name2url = {name:url for name,url in resolved}\n        manifest = []\n\n        for idx, name in enumerate(selected, 1):\n            url = name2url.get(name)\n            soup = get_soup(url)\n            if not soup:\n                log(f\"- {idx}/{len(selected)} {name}: pagina non caricata, salto.\")\n                continue\n\n            h1 = soup.find(\"h1\")\n            dirname = slugify(h1.get_text(strip=True)) if h1 else slugify(name)\n            kdir = os.path.join(OUTPUT_DIR, dirname)\n            os.makedirs(kdir, exist_ok=True)\n\n            # descrizione\n            desc_container = soup.select_one(\".gb-text-and-link\")\n            paras = [p.get_text(\" \", strip=True) for p in (desc_container.find_all(\"p\") if desc_container else []) if p.get_text(\" \", strip=True)]\n            desc = \"\\n\\n\".join(paras).strip()\n            with io.open(os.path.join(kdir, \"descrizione.txt\"), \"w\", encoding=\"utf-8\") as f:\n                f.write(desc)\n\n            # immagini\n            wrappers = soup.select(\".gb-media-wrapper\")\n            anchors = []\n            for w in wrappers:\n                anchors.extend(w.select(\"a.gb-item-link\"))\n\n            seen = set(); ordered = []\n            for a in anchors:\n                href = a.get(\"href\")\n                if not href: continue\n                abs_url = urljoin(url, href)\n                if abs_url.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".webp\")) and abs_url not in seen:\n                    seen.add(abs_url)\n                    ordered.append(abs_url)\n                if len(ordered) >= img_num.value:\n                    break\n\n            saved = 0\n            for i, img_url in enumerate(ordered, 1):\n                dest = os.path.join(kdir, f\"{i:02d}{infer_ext(img_url)}\")\n                if download_image(img_url, dest):\n                    saved += 1\n                time.sleep(0.1)\n\n            manifest.append({\"kitchen_name\": dirname, \"url\": url, \"description_chars\": len(desc), \"images_saved\": saved})\n            log(f\"- {idx}/{len(selected)} {dirname}: trovate {len(ordered)}, salvate {saved}\")\n            time.sleep(0.2)\n\n        # manifest & zip\n        man_path = os.path.join(OUTPUT_DIR, \"manifest.csv\")\n        with io.open(man_path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n            w = csv.DictWriter(f, fieldnames=[\"kitchen_name\",\"url\",\"description_chars\",\"images_saved\"])\n            w.writeheader(); w.writerows(manifest)\n\n        with zipfile.ZipFile(ZIP_PATH, \"w\", zipfile.ZIP_DEFLATED) as z:\n            for root, _, files in os.walk(OUTPUT_DIR):\n                for file in files:\n                    full = os.path.join(root, file)\n                    rel = os.path.relpath(full, OUTPUT_DIR)\n                    z.write(full, arcname=os.path.join(\"creo_cucine\", rel))\n\n        log(f\"ZIP pronto: {ZIP_PATH}\")\n\n        # Link compatibile con Voil\u00e0/Binder:\n        # Voil\u00e0 serve il working dir sotto /files/\n        download_area.value = f'<a href=\"files/{os.path.basename(ZIP_PATH)}\" download style=\"font-weight:bold;\">\u2b07\ufe0f Scarica ZIP</a>'\n\n    except Exception:\n        log(\"Errore:\\n\" + traceback.format_exc())\n\nbtn_run.on_click(run_scraping)\n\nui = widgets.VBox([\n    widgets.HTML(\"<h3>Seleziona le cucine da scaricare</h3>\"),\n    select_all,\n    box_checks,\n    img_num,\n    btn_run,\n    download_area,\n    widgets.HTML(\"<hr>\"),\n    widgets.HTML(\"<b>Log</b>\"),\n    out\n])\n\ndisplay(ui)\n"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.x"}}, "nbformat": 4, "nbformat_minor": 5}